<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Zero-shot Text-Driven Avatar Generation based on Depth-conditioned Diffusion Model.">
  <meta name="keywords" content="DcHuman, Diffusion, Zero-shot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DcHuman:Zero-shot Text-Driven Avatar Generation based on Depth-conditioned Diffusion Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://hazelWang0.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <!-- <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DcHuman:Zero-shot Text-Driven Avatar Generation based on Depth-conditioned Diffusion Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hazelWang0.github.io">Ji Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a >Sen Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Zhiwen Jiang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Zhifen Xie</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Mentian Li</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Special Effects Engineering Research Cente</span>
          </div>

          <!-- <div class="column has-text-centered"> -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a  
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/result.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DcHuman</span> generates 3D avatar models freely with texts. 
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a avatar generation framework based on a Deep Conditional Diffusion Model (DCDM) called DcHuman.   
          </p>
          <p>
            Our approach comprises two main components: the human body's implicit neural field and the Deep Conditional Diffusion Model itself. 
            The overall generation process consists of two stages: conditional human body generation and iterative texture refinement.  
          </p>
          <!-- <p>
            In the first stage, We leverages the robust semantic priors of a pre-trained Deep Conditional Diffusion Model, 
            which provides global semantic feature supervision, and the generation of human body features with global consistency is guided using Score Distillation Sampling (SDS) as proposed in [4]. Additionally, an implicit human neural rendering module is employed based on NeuS [11]. This module comprises two parts: human body reconstruction and style generation. A shared weighted function is used to constrain human features, integrating depth information into the diffusion model's latent space. This not only resolves geometric ambiguities but also constrains the diffusion model's generation distribution, enhancing its capability to generate detailed human features.
          </p>
          <p>
            In the second stage, we devised a collaborative optimization strategy. Pre-trained Deep Conditional Diffusion Model's extensive semantic priors and a conditional denoising restoration strategy are used to recover high-precision texture-guided images. This optimizes the texture representation of the implicit human neural field. This stage capitalizes on color information obtained in the first stage as texture priors. Collaborating with depth information, it narrows down the diffusion model's image distribution to a plausible texture range. This iterative process elevates texture quality and global consistency, culminating in refined high-quality human models.
          </p> -->
          <p>
            We show the effectiveness of DcHuman through experiments in avatar model generation. It proves that compared to prior methods, the generated models exhibit higher texture quality, improved feature consistency, and a greater sense of realism, all while staying faithful to given textual input and maintaining prescribed human postures and forms.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Framwork. -->
    <div class="column">
      <div class="content">
        <h2 class="title is-3">Framework</h2>
        <p>
          Using <i>DcHuman</i> In the first stage, we harnesses the powerful semantic priors of a pre-trained deep conditional diffusion model 
          to provide global semantic feature supervision. Guided by score distillation sampling, the model achieves human feature generation with global consistency.
           Simultaneously, we employs an implicit human neural rendering module based on NeuS. Comprising human reconstruction and style generation modules, 
           a shared weighted function is used to constrain human features while incorporating depth information into the latent space of the diffusion model.

          In the second stage, we reconstructs the texture generation module. Utilizing the color information obtained from the first stage as texture priors, 
          the same pre-trained deep conditional diffusion model is employed. Through a strategy of conditional denoising and restoration, high-precision texture guidance 
          images are recovered as part of an iterative refinement process for DcHuman's texture.
        </p>
        <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/dollyzoom-stacked.mp4"
                  type="video/mp4">
        </video> -->
        <img src="./static/images/Framework.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
      </div>
    </div>
    <!-- Framwork. -->


    <!-- Display. -->
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            we conducts a comparative and qualitative analysis with AvatarCLIP, DreamFusion, and SJC methods.  
            concerning the geometric precision and texture realism in full-body avatar generation, our novel approach accurately represents character attributes and 
            exhibits significant improvement over existing methods in terms of global texture consistency. Concurrently, it outperforms 
            existing methods in identity generation, stylization, and feature editing tasks. 
          </p>
          <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video> -->
          <img src="./static/images/result2.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        </div>
      </div>
    </div>
    <!-- Display. -->
  

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         >
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Page borrow from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
